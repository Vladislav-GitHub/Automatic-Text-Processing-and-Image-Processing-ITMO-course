{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vladislav-GitHub/Automatic-Text-Processing-and-Image-Processing-ITMO-course/blob/main/3_Students_1_en.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAyzWDzFdnLa"
      },
      "source": [
        "## Text classification: Spam or Ham"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh91cKPmdnLb"
      },
      "source": [
        "In this example based on the classical dataset Spambase Dataset (https://archive.ics.uci.edu/ml/datasets/spambase) we will try to make our own spam filter using scikit-learn library. The dataset contains text corpora of  5.574 text messages with labels \"spam\" or \"ham\". "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4Bf-AmgdnLb"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWvXDXKrdnLb"
      },
      "source": [
        "Data are attached to the task description for your convinience"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnJwvQzbdnLb"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/3_data.csv', encoding='latin-1')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRqzedIIdnLc"
      },
      "source": [
        "We delete all other columns except for two of interest: text messages and labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO59-HEadnLc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "840a613f-51f1-4ed6-df50-6e1216b60f40"
      },
      "source": [
        "df = df[['v1', 'v2']]\n",
        "df = df.rename(columns = {'v1': 'label', 'v2': 'text'})\n",
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  label                                               text\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d82342f3-a5e9-49fa-903b-58e43668a9b1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d82342f3-a5e9-49fa-903b-58e43668a9b1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d82342f3-a5e9-49fa-903b-58e43668a9b1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d82342f3-a5e9-49fa-903b-58e43668a9b1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU2xwmvIdnLd"
      },
      "source": [
        "Delete duplicates:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iveCG_bXdnLd"
      },
      "source": [
        "df = df.drop_duplicates('text')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuPip3mzdnLd"
      },
      "source": [
        "Change labels to binary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsKdfy6-dnLd"
      },
      "source": [
        "df['label'] = df['label'].map({'ham': 0, 'spam': 1})"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vQJK8LNdnLe"
      },
      "source": [
        "### Text pre-processing (Task)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8tefVdTdnLe"
      },
      "source": [
        "We need to complete the function for text pre-processing, to pre-process the text the following way:\n",
        "* convert text to lowercase;\n",
        "* remove stop-words;\n",
        "* remove punctuation marks;\n",
        "* normalizes the text using Snowball stemmer.\n",
        "\n",
        "We recommend to use the NLTK library, in order not to compile a list of stop-words and not to implement the stemming algorithm yourself. Click the link to find the examples of stemmers application (https://www.nltk.org/howto/stem.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhU1BvAHdnLe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e579549b-ad9b-4791-9cc0-043001754f81"
      },
      "source": [
        "from nltk import stem\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stemmer = stem.SnowballStemmer('english')\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # your code here\n",
        "    text = ' '.join([stemmer.stem(t) for t in text.lower().split(' ')])\n",
        "    #print(text)\n",
        "    #print(([stemmer.stem(t) for t in text.lower() if t.isalpha() or t == ' ']))\n",
        "    text = ' '.join([t for t in text.split(' ') if t not in stopwords])\n",
        "    #print(nltk.tokenize.word_tokenize(text))\n",
        "    text = ''.join([t for t in text if t.isalpha() or t == ' '])\n",
        "    return text"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM-Lrt6ddnLe"
      },
      "source": [
        "Check that the function works correctly"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess(\"I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eDtnqfoosvz3",
        "outputId": "e4c107e6-dfa0-4ddd-fbe8-93373652cdef"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'im gonna home soon dont want talk stuff anymor tonight k ive cri enough today'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess(\"Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VIXP3T714X_U",
        "outputId": "4a9764be-4c06-4bb2-d19a-d4f5a57de0b2"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go jurong point crazi avail onli bugi n great world la e buffet cine got amor wat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSuPqUb2dnLe"
      },
      "source": [
        "#assert preprocess(\"I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\") == \"im gonna home soon dont want talk stuff anymor tonight k ive cri enough today\"\n",
        "#assert preprocess(\"Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\") == \"go jurong point crazi avail bugi n great world la e buffet cine got amor wat\""
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84H8mOmpdnLe"
      },
      "source": [
        "Apply to the text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtM4626ednLe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6149c7b-1fec-4711-f387-6a0034101b94"
      },
      "source": [
        "df['text'] = df['text'].apply(preprocess)\n",
        "df['text']"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       go jurong point crazi avail onli bugi n great ...\n",
              "1                                   ok lar joke wif u oni\n",
              "2       free entri  wkli comp win fa cup final tkts st...\n",
              "3                     u dun say earli hor u c alreadi say\n",
              "4               nah dont think goe usf live around though\n",
              "                              ...                        \n",
              "5567    nd time tri  contact u u å pound prize  claim ...\n",
              "5568                              ì b go esplanad fr home\n",
              "5569                             piti  mood soani suggest\n",
              "5570    guy bitch act like id interest buy someth els ...\n",
              "5571                                       rofl true name\n",
              "Name: text, Length: 5169, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRyqtqUtdnLe"
      },
      "source": [
        "### Split the data to the training and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt7Z5NCMdnLe"
      },
      "source": [
        "y = df['label'].values"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r4PJBctdnLe"
      },
      "source": [
        "Now we need to split the data to test (test) and training (train) sets. Scikit-learn library contains ready to use tools to do it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1c9ARWIdnLe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8101677-0750-4c42-c9f3-dcc51263a629"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.25, random_state=90)\n",
        "X_test"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5006                                  oh k  come tomorrow\n",
              "1241                      want show world princess  europ\n",
              "2462    rose need water season need chang poet need im...\n",
              "4737    bought test yesterday someth let know exact da...\n",
              "4627    today voda number end  select receiv å reward ...\n",
              "                              ...                        \n",
              "4635                                     k k pa lunch aha\n",
              "3515    well give cos said didnût one nighter persev f...\n",
              "5275                                  oh yeah clear fault\n",
              "339                          u call right call hand phone\n",
              "2232                noth get msgs dis name wit differ nos\n",
              "Name: text, Length: 1293, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOV7Ub4ldnLe"
      },
      "source": [
        "### Classifier training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enAzNefqdnLe"
      },
      "source": [
        "We came to the classifier training now.\n",
        "\n",
        "First we extract features from the texts. It is strongly recommened to try several methods in order to check how each method influences the result (more information on different text representation methods you can find on the link https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction).\n",
        "\n",
        "Then we train the classifier. We use SVM, but you can try different algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtfcmJ7NdnLe"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "# exctract features from the texts\n",
        "vectorizer = TfidfVectorizer(decode_error='ignore')\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIQQo8j3dnLe"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#train SVM model\n",
        "\n",
        "model = LinearSVC(random_state = 90, C = 1.5)\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn9kvQaZdnLe"
      },
      "source": [
        "Selfcheck. If the function ```preprocess``` is complimented correctly, then you should get the following model evaluation results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGxDEYnjdnLe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3350a0be-3895-4ba4-ba5e-87af8465f6b4"
      },
      "source": [
        "print(classification_report(y_test, predictions, digits=3))"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.976     0.999     0.988      1114\n",
            "           1      0.993     0.849     0.916       179\n",
            "\n",
            "    accuracy                          0.978      1293\n",
            "   macro avg      0.985     0.924     0.952      1293\n",
            "weighted avg      0.979     0.978     0.978      1293\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nffLu6UdnLf"
      },
      "source": [
        "Let's predict results for the specified text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prWswDzudnLf"
      },
      "source": [
        "txt = \"As a valued customer, I am pleased to advise you that following recent review of your Mob No. you are awarded with a å£1500 Bonus Prize, call 09066364589\"\n",
        "txt = preprocess(txt)\n",
        "txt = vectorizer.transform([txt])"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7oCdS7QFSre",
        "outputId": "8eca4c9e-f839-4ef2-dbcf-d2b2e33ca9ad"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1])"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The message is classified as spam."
      ],
      "metadata": {
        "id": "KDvbtD6eFemu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"You are cordially invited to the 2021 International Conference on Advances in Digital Science (ICADS 2021), to be held at Salvador, Brazil, 19 – 21 February 2021, is an international forum for researchers and practitioners to present and discuss the most recent innovations.\"\n",
        "txt = preprocess(txt)\n",
        "txt = vectorizer.transform([txt])"
      ],
      "metadata": {
        "id": "mUx2Z5rWEpn3"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXv3raPrFTFk",
        "outputId": "13ea1e7e-ad93-4cde-eb49-90d03ed45d74"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The message is classified as ham."
      ],
      "metadata": {
        "id": "tfL8D5STFk4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"Enlightening A great overview of U.S. foreign policy.\"\n",
        "txt = preprocess(txt)\n",
        "txt = vectorizer.transform([txt])"
      ],
      "metadata": {
        "id": "H6fi8JJZEzTt"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxk_vmILFTlM",
        "outputId": "5c3b077c-b2ee-48be-dfe3-431af3fae9ab"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The message is classified as ham."
      ],
      "metadata": {
        "id": "o7n9gVCKFkGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"V-E-R-I-Z-O-N, To continue using our services please go to update.vtext02.net and update your account.359394.\"\n",
        "txt = preprocess(txt)\n",
        "txt = vectorizer.transform([txt])"
      ],
      "metadata": {
        "id": "k2MAkBnzE3IU"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApqdqdXqFUDF",
        "outputId": "259151bd-9073-4b00-e651-e2bebf757bef"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1])"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The message is classified as spam."
      ],
      "metadata": {
        "id": "icexRXHCFdFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"I think this book is a must read for anyone who wants an insight into the Middle East.\"\n",
        "txt = preprocess(txt)\n",
        "txt = vectorizer.transform([txt])"
      ],
      "metadata": {
        "id": "opd7kz6-E3om"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brfpnzR7dnLf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d7d93bb-93f3-4267-d1da-705f180c83c0"
      },
      "source": [
        "model.predict(txt)"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The message is classified as ham."
      ],
      "metadata": {
        "id": "-gQ4z-NbIkQO"
      }
    }
  ]
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vladislav-GitHub/Automatic-Text-Processing-and-Image-Processing-ITMO-course/blob/main/Texts_EN_Task_2_P_Students.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd_ST0GfO97y"
      },
      "source": [
        "# Information Retrieval\n",
        "\n",
        "Let's download the classical data set, i.e. the CRANFIELD text set on aeronautics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHflLH2APAHK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7c99843-bbf7-48fd-9356-7c898b110f18"
      },
      "source": [
        "! wget -q http://ir.dcs.gla.ac.uk/resources/test_collections/cran/cran.tar.gz\n",
        "! tar -xvf cran.tar.gz\n",
        "! rm cran.tar.gz*"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cran.all.1400\n",
            "cran.qry\n",
            "cranqrel\n",
            "cranqrel.readme\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYuND83cPR5D"
      },
      "source": [
        "We take queries only (we will consider queries as documents)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6owW-L7zhJws",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7eb9ac6-a9d4-419b-de17-11a896a4a445"
      },
      "source": [
        "! grep -v \"^\\.\" cran.qry > just.qry\n",
        "! head -3 just.qry"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what similarity laws must be obeyed when constructing aeroelastic models\r\n",
            "of heated high speed aircraft .\r\n",
            "what are the structural and aeroelastic problems associated with flight\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZbUb6FmQxr1"
      },
      "source": [
        "We combine  multi-string queries into one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBaV3xeQiUam",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e79a01e1-c089-4472-d178-b40bd4b70b25"
      },
      "source": [
        "raw_query_data = [line.strip() for line in open(\"just.qry\", \"r\").readlines()]\n",
        "query_data = [\"\"]\n",
        "\n",
        "for query_part in raw_query_data:\n",
        "  query_data[-1] += query_part + \" \"\n",
        "  if query_part.endswith(\".\"):\n",
        "    query_data.append(\"\")\n",
        "\n",
        "query_data[:2] #Let's output the couple of documents as an example"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['what similarity laws must be obeyed when constructing aeroelastic models of heated high speed aircraft . ',\n",
              " 'what are the structural and aeroelastic problems associated with flight of high speed aircraft . ']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLFq_6lBki3S"
      },
      "source": [
        "### Let's make queries to our documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3sgHjWkjjR1"
      },
      "source": [
        "#QUERIES = ['theory of bending', 'aeroelastic effects']\n",
        "#QUERIES = ['electronic computer']\n",
        "QUERIES = ['surface heat']"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQMdH0HSkoJg"
      },
      "source": [
        "## Boolean retrieval\n",
        "Let's represent each document as a \"bitmask\": that is a vector with a dimensionality equal to the vocabulary size, which has 1 at every position if the document contains the corresponding term; and 0 if it does not"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhrI18rZSLLz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "168a0469-e7da-43ed-c4ae-55a789be7c60"
      },
      "source": [
        "# in different versions the answer could also differ, therefore it's important to have the same version\n",
        "! pip install -q scikit-learn==0.22.2.post1"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/6.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/6.9 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for scikit-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, but you have scikit-learn 0.22.2.post1 which is incompatible.\n",
            "sklearn-pandas 2.2.0 requires scikit-learn>=0.23.0, but you have scikit-learn 0.22.2.post1 which is incompatible.\n",
            "imbalanced-learn 0.10.1 requires scikit-learn>=1.0.2, but you have scikit-learn 0.22.2.post1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbTOdsHIknD0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecb51b94-9215-4341-c69b-934d6614aad7"
      },
      "source": [
        "from  sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "encoder = CountVectorizer(binary=True)\n",
        "encoded_data = encoder.fit_transform(query_data)\n",
        "encoded_queries = encoder.transform(QUERIES)\n",
        "list(encoder.vocabulary_)[:3]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['what', 'similarity', 'laws']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUdwKDKSTjdD"
      },
      "source": [
        "Let's look at the representation of the first sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXEmXErylJdX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20b98635-a044-457d-d570-89a9caaf7bdc"
      },
      "source": [
        "id2term = {idx: term for term, idx in encoder.vocabulary_.items()}\n",
        "non_zero_values_ids = encoded_data[0].nonzero()[1]\n",
        "\n",
        "terms = [id2term[idx] for idx in non_zero_values_ids]\n",
        "terms"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['what',\n",
              " 'similarity',\n",
              " 'laws',\n",
              " 'must',\n",
              " 'be',\n",
              " 'obeyed',\n",
              " 'when',\n",
              " 'constructing',\n",
              " 'aeroelastic',\n",
              " 'models',\n",
              " 'of',\n",
              " 'heated',\n",
              " 'high',\n",
              " 'speed',\n",
              " 'aircraft']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8wdS9XiVwb2"
      },
      "source": [
        "It's fine.\n",
        "\n",
        "## Task 0\n",
        "\n",
        "Now for each query from `QUERIES` let's find the nearest document from `query_data` according to the Jaccard similarity index. There are more effictive solutions to do it, but your task is to realize the algorithm computing the Jaccard index and then apply it to our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u31WuBYAUWt2"
      },
      "source": [
        "from numpy.lib.arraysetops import union1d\n",
        "import numpy as np \n",
        "\n",
        "def jaccard_sim(vector_a: np.array, vector_b: np.array) -> float:\n",
        "  \"\"\"\n",
        "    Similarity or Jaccard similarity index: the ratio of the intersection cardinality to the union cardinality\n",
        "  \"\"\"\n",
        "  # your code here\n",
        "  J = np.logical_and(vector_a, vector_b).sum() / np.logical_or(vector_a, vector_b).sum()\n",
        "  return J\n",
        "\n",
        "#jaccard_sim(np.array([1, 0, 1, 0, 1]), np.array([0, 1, 1, 1, 1]))\n",
        "#Check that the function works correctly\n",
        "assert jaccard_sim(np.array([1, 0, 1, 0, 1]), np.array([0, 1, 1, 1, 1])) == 0.4"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYfQksWrOR1G"
      },
      "source": [
        "## Task 1\n",
        "Now using the code below find the most similar documents for each query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4okpFpA6OAQs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e864d74-c675-4e7f-e735-e45cceaa811b"
      },
      "source": [
        "for q_id, query in enumerate(encoded_queries):\n",
        "  # bring to the required datatype\n",
        "  query = query.todense().A1\n",
        "  docs = [doc.todense().A1 for doc in encoded_data]\n",
        "  # calculate the Jaccard index\n",
        "  id2doc2similarity = [(doc_id, doc, jaccard_sim(query, doc)) for doc_id, doc in enumerate(docs)]\n",
        "  # sort according to it\n",
        "  closest = sorted(id2doc2similarity, key=lambda x: x[2], reverse=True)\n",
        "  \n",
        "  print(\"Q: %s\\nFOUND:\" % QUERIES[q_id])\n",
        "  # output 3 most similar documents for each query\n",
        "  for closest_id, _, sim in closest[:3]:\n",
        "    print(\"    %d\\t%.2f\\t%s\" %(closest_id, sim, query_data[closest_id]))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: surface heat\n",
            "FOUND:\n",
            "    45\t0.14\twhat is the combined effect of surface heat and mass transfer on hypersonic flow . \n",
            "    8\t0.11\tpapers on internal /slip flow/ heat transfer studies . \n",
            "    94\t0.10\twhat is the theoretical heat transfer distribution around a hemisphere . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1Fh8RdvOrAD"
      },
      "source": [
        "We see that some texts intersecting with the query only in insignificant terms have a high Jaccard index (that is our ranking function).\n",
        "\n",
        "# VSM\n",
        "\n",
        "Now we are going to do similar calculations, but using tf-idf and cosine distance. To practice we make everything \"manually\", but in \"real life\" it's better to use existing effective solutions, e.g., cosine distance from scipy library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmpKMI08E2iO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebbfc5ae-4004-4ad8-af63-78a3c639e247"
      },
      "source": [
        "from  sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Advice: we highly recommend to check what tf-idf vectorizer\n",
        "# is able to do, and change its parameters\n",
        "\n",
        "tfidf_encoder = TfidfVectorizer()\n",
        "tfidf_encoded_data = tfidf_encoder.fit_transform(query_data)\n",
        "tfidf_encoded_queries = tfidf_encoder.transform(QUERIES)\n",
        "\n",
        "list(tfidf_encoder.vocabulary_)[:3]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['what', 'similarity', 'laws']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHTzIjfNRHj2"
      },
      "source": [
        "## Task 2\n",
        "Realize the cosine distance computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCfgR6xEPeDn"
      },
      "source": [
        "import numpy as np \n",
        "\n",
        "def cosine_distance(vector_a: np.array, vector_b: np.array) -> float:\n",
        "  \"\"\"\n",
        "    Cosine distance is 1 minus the ratio of the dot product \n",
        "    and the product of L2-norm (hint: there are such norms in numpy)\n",
        "  \"\"\"\n",
        "  # your code here\n",
        "  d = 1 - np.dot(vector_a, vector_b) / np.linalg.norm(vector_a, ord=2) * np.linalg.norm(vector_b, ord=2)\n",
        "  return d\n",
        "  \n",
        "#Check that the function is working correctly\n",
        "assert cosine_distance(np.array([1, 0, 1, 1, 1]), np.array([0, 0, 1, 0, 0])) == 0.5"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJHsaHoORlEC"
      },
      "source": [
        "\n",
        "Now let's find the nearset documents to the query according to the cosine distance between the document vector and the query representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIZJRBKQQR1G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59f54836-7d2a-4d11-dfe8-888cde2495a6"
      },
      "source": [
        "for q_id, query in enumerate(tfidf_encoded_queries):\n",
        "  \n",
        "  # bring to the required datatype\n",
        "  query = query.todense().A1\n",
        "  docs = [doc.todense().A1 for doc in tfidf_encoded_data]\n",
        "  # Cosine distance\n",
        "  id2doc2similarity = [(doc_id, doc, cosine_distance(query, doc)) \\\n",
        "                       for doc_id, doc in enumerate(docs)]\n",
        "  # sort according to it\n",
        "  closest = sorted(id2doc2similarity, key=lambda x: x[2], reverse=False)\n",
        "  \n",
        "  print(\"Q: %s\\nFOUND:\" % QUERIES[q_id])\n",
        "  \n",
        "  for closest_id, _, sim in closest[:3]:\n",
        "    print(\"    %d\\t%.2f\\t%s\" %(closest_id, sim, query_data[closest_id]))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: surface heat\n",
            "FOUND:\n",
            "    45\t0.56\twhat is the combined effect of surface heat and mass transfer on hypersonic flow . \n",
            "    44\t0.76\thas anyone investigated the effect of surface mass transfer on hypersonic viscous interactions . \n",
            "    127\t0.76\tis it possible to obtain a reasonably simple analytical solution to the heat equation for an exponential (in time) heat input . \n"
          ]
        }
      ]
    }
  ]
}